# Form Builder AI: A Case Study in Knowing When to Pause

> **Quick Take:** A real-world example of collaborative product leadership. I partnered with our Innovation team to explore an AI-generated form builder — and helped guide the decision *not* to release it based on real user behavior and risk context.

---

## Overview

At SafetyChain, I collaborated with the Innovation team as they developed an AI-powered form builder. The concept was compelling: users could generate digital forms by uploading images or writing simple prompts, which would then be translated into structured, functional forms.

The tool was especially promising for first-time builds. It was fast, intuitive, and flexible in ways that sparked early excitement. But as the team gathered feedback, we saw that the flexibility became a liability during editing. The number of directions a prompt could take a user created too many paths and introduced ambiguity where precision was needed.

---

## What We Learned

- Initial creation worked well, but editing felt unpredictable. Users struggled to modify forms consistently, unsure of how prompts would behave across sessions.
- The wide open nature of prompts added risk. In an industry where accuracy and repeatability matter more than novelty, this flexibility worked against user confidence.
- One subtle but significant barrier was **terminology disconnect**. For example, the term `value` appeared throughout the form’s backend logic, but didn’t match how users thought about the data they were entering. This made basic tasks like editing a dropdown confusing, and added friction in a place where AI was supposed to reduce it.
- Adoption readiness isn’t guaranteed. Many of our users were still building comfort with standard software workflows, and this leap felt too fast.

---

## The Decision

Rather than try to force adoption or simplify the interface prematurely, we made the call to keep the AI form builder in alpha and continue testing it with a small group of early adopters. The infrastructure remains in place, ready to reintroduce when both the product and the users are ready.

We pivoted to explore more constrained AI use cases where:

- The user already expects variation (like filtering data or generating visualizations)
- The outcomes are easier to guide and QA
- The user environment supports more experimentation without loss of trust

---

## Reflections

This project grounded some of the most important instincts I’ve developed as a product leader: knowing when *not* to ship, listening closely to user signals, and understanding that clarity and trust must come before novelty. It’s easy to get caught up in the excitement of AI, but every new tool changes the user paradigm. If you aren’t careful, you risk introducing confusion before building confidence.

Working closely with the Innovation team gave me the chance to apply these instincts in a meaningful way. I helped shape the decision to pause full rollout not because the technology wasn’t exciting, but because the experience wasn’t reliable enough for our users yet. In a regulated, risk-averse environment, even minor inconsistencies in language or behavior can erode trust fast.

This experience reinforced a principle I carry into all AI product work: **intelligence only matters when it feels usable**. Clarity between what a user sees and what a system expects isn’t just a UX issue — it’s foundational to trust.

One of the hardest parts of product leadership is knowing when to say no, even when a solution is promising. That decision requires more than good judgment. It requires deep user empathy, cross-functional alignment, and the confidence to hold back when pushing forward would cost more than it gives. This was one of those moments, and I’m proud we made the call we did.

---

[⬅️ Back to main portfolio](../README.md)
